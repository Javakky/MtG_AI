# Ensemble Determinization in Monte Carlo TreeSearch for the Imperfect Information Card GameMagic: The Gathering

---

## Abstract

MtG(Magic: The Gathering) を改良したゲームについてMCTS(モンテカルロ木探索)を調査した。

---

### MtGのゲーム木にある特徴

- 不完備情報ゲームである
- 不確定情報なゲームである

<small>※ただしこの研究では、非公開情報及び不確定情報はすべてのプレイヤーが知っているものとする</small>

---

#### 不完備情報ゲーム

ゲームのルールあるいはプレイするのに必要な情報がプレイヤー間で共有されていないゲーム[*](https://www.wikiwand.com/ja/%E4%B8%8D%E5%AE%8C%E5%82%99%E6%83%85%E5%A0%B1%E3%82%B2%E3%83%BC%E3%83%A0)
<p>
MtGでは非公開領域(山札・相手の手札)の存在により、プレイヤー間で使える情報が共有されていない
</p>

---

#### 不確定情報ゲーム

偶然に左右され、自分で完全に次の手を決められないゲーム[*](http://lis2.huie.hokudai.ac.jp/file/game_ai.pdf)
<p>
MtGではシャッフルされた山札からカードを引くことで手札に加えていくため、次に可能な手がランダムに決まる
</p>

---

### 研究概要

この研究には以下を検証する

- 報酬の減衰
- 枝刈りの戦略
- 手の決定プロセス
    * 2分決定木の生成手順の分解
      <br>

これにより、通常の固定デッキを用いたMCTSと比較してパフォーマンスを大幅に改善した

---

## MCTSとは

モンテカルロ木探索

ここでは他の探索法と比較しながらモンテカルロ木探索について説明する

---

### ミニマックス法

二人零和有限確定情報ゲームで用いられる
<p>
ゲーム木に対し以下の基準で最も評価状態が高いノードを選択する
</p>

- 自局面のノードはその子ノードの状態評価の最大値を状態価値とする
- 相手局面のノードはその子ノードの状態評価の最小値を状態価値とする

---

#### 評価方法

- ゲームの終了時 ー 勝ち：1、引き分け：0、負け：-1
- ゲームの途中 ー 子のノードを反転し、最大のものを選ぶ
    * 自分の番では値が正
        - 最も(自分にとって)評価値が高いものを選ぶ
    * 相手の番では値が負
        - 最も(自分にとって)評価値が低いものを選ぶ

この評価方法を用いるミニマックス法を**ネガマックス法**ともいう

---

### アルファベータ法

ミニマックス法に対して枝刈りを行う

---

#### αカット

現在の**自局面**の評価値を**α値**、その子の中で、ある**相手局面**の評価値を**β値**と呼ぶ

- 1つの相手局面をすべて探索した場合、その評価値をα値とする
- 別の相手局面を探索する際に、そのある子(自局面)の評価値を求めβ値とする
- β値がα値より小さい場合、現在探索中の相手局面は**選ばない**ため探索を中止する

<small>※もし自分がその相手局面を選択した場合、相手はβ値以下の値しか選ばないため最終結果がα値より小さくなる</small>

---

#### βカット

現在の**相手局面**の評価値を**β値**、その子の中で、ある**自局面**の評価値を**α値**と呼ぶ

- 1つの自局面をすべて探索した場合、その評価値をβ値とする
- 別の自局面を探索する際に、そのある子(相手局面)の評価値を求めα値とする
- α値がβ値より大きい場合、現在探索中の自局面は**選ばれない**ため探索を中止する

<small>※もし相手がその自局面を選択した場合、自分はα値以上の値しか選ばないため、最終結果がα値より大きくなる(相手にとって不利になる)</small>

---

### 原始モンテカルロ法

状態価値をランダムシミュレーションによって決定する -> 全探索を行わない

それぞれのノードに対しn回ランダムにプレイアウト(終端まで探索)したとき、その**評価値の合計**を評価値とする

---

### モンテカルロ木探索

原始モンテカルロ法では、相手が最善手を選び続けた場合に評価値が低くなる<small>※自分は平均的により高いものを選んでいるため</small>

以下の4つの手順により、より有望な手を探索する方法

- 選択
- 評価
- 展開
- 更新

---

#### 初期状態

現在局面とその子はそれぞれw、nを持つ

- w：累計価値
- n：試行回数

初期状態のゲーム木は**現在局面**と(考えうる)**次の一手**のみからなる

---

#### 選択

リーフに到達するまで移動を繰り返す

- 試行回数が0回のノードがあればそれを選択する
- 試行回数が0回のノードが複数あるなら、先に見つけた方から選択する
- UCB1の値が高い手を選ぶ

---

##### UCB1

$$UCB1 = w/n + \sqrt{\frac {2*\log(t)} n}$$

勝率にバイアス(試行回数の少なさ)を足したもの

<small>
<a href="https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&page_id=13&block_id=8&item_id=106513&item_no=1">多腕バンディットアルゴリズムのMCTSへの応用と性能の分析</a>
</small>

---

#### 評価

探索が終端に到達した際に**プレイアウト**を行い、**累計報酬**に**価値**を**試行回数**に**1**を加算する

---

#### 展開

あるリーフの試行回数が任意の回数以上になった時、ゲーム木にそのリーフの次の一手を追加する

---

#### 更新

ルートに到達するまで親のノードに値を加算していく

- 累計価値：算出した価値
- 試行回数：1

---

#### 手の決定

十分に試行した後、次の一手から最も試行回数の大きい手を選ぶ

---